{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OoiJooYee/ActionDetectionforSignLanguage/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ STEP 1: 卸载有冲突的版本\n",
        "!pip uninstall -y mediapipe protobuf numpy\n",
        "\n",
        "!pip install mediapipe==0.10.9 protobuf==3.20.3 numpy==1.23.5 --quiet\n",
        "\n",
        "!pip install scikit-learn==1.3.0 matplotlib opencv-python tensorflow==2.16.1 --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "WF-y6RAESFwh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🔍 检查关键包的导入情况...\")\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(f\"✅ NumPy 版本: {np.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ NumPy 导入失败: {e}\")\n",
        "\n",
        "try:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import sklearn\n",
        "    print(f\"✅ scikit-learn 版本: {sklearn.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ scikit-learn 导入失败: {e}\")\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"✅ TensorFlow 版本: {tf.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ TensorFlow 导入失败: {e}\")\n"
      ],
      "metadata": {
        "id": "vSNa1aEySGwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy==1.26.4 --quiet\n",
        "\n",
        "print(\"✅ NumPy 已升级到 1.26.4\")\n"
      ],
      "metadata": {
        "id": "SXYXNCZiS3Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🧪 测试导入...\")\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(f\"✅ NumPy 版本: {np.__version__}\")\n",
        "\n",
        "    # 测试 numpy.dtypes 是否可用\n",
        "    if hasattr(np, 'dtypes'):\n",
        "        print(\"✅ NumPy dtypes 可用\")\n",
        "    else:\n",
        "        print(\"⚠️  NumPy dtypes 不可用\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ NumPy 错误: {e}\")\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"✅ TensorFlow 版本: {tf.__version__}\")\n",
        "\n",
        "    # 测试基本功能\n",
        "    test_tensor = tf.constant([1, 2, 3])\n",
        "    print(f\"✅ TensorFlow 基本功能正常: {test_tensor}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ TensorFlow 错误: {e}\")\n",
        "\n",
        "try:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import sklearn\n",
        "    print(f\"✅ scikit-learn 版本: {sklearn.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ scikit-learn 错误: {e}\")\n",
        "\n",
        "try:\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    print(\"✅ Keras utils 导入成功\")\n",
        "\n",
        "    # 测试 to_categorical 功能\n",
        "    test_labels = to_categorical([0, 1, 2], num_classes=3)\n",
        "    print(f\"✅ to_categorical 功能正常: {test_labels.shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Keras utils 错误: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🎯 如果所有测试都通过，你可以继续进行手势识别项目！\")\n",
        "print(\"🔄 如果还有错误，请重启运行时后再次运行这个代码。\")\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "8JqZUzY7S_DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装兼容版本的 numpy 和 tensorflow\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5 --force-reinstall\n",
        "!pip install --upgrade --no-cache-dir tensorflow\n"
      ],
      "metadata": {
        "id": "8Ql2HQm483dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(\"✅ TensorFlow version:\", tf.__version__)\n",
        "print(\"✅ NumPy version:\", np.__version__)"
      ],
      "metadata": {
        "id": "eE-uq8VE8-8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5\n",
        "!pip install tensorflow==2.13.0  # 这个版本对 numpy 1.23.5 最稳定"
      ],
      "metadata": {
        "id": "7XJFZ_gm9F6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK45ixAa6jVm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_L-V89DPPki"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json here\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vVk5sNitPRIg"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d risangbaskoro/wlasl-processed\n",
        "!unzip wlasl-processed.zip -d wlasl_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic9jNpGQl3w"
      },
      "outputs": [],
      "source": [
        "#1-------------------------------------\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the WLASL metadata file (usually comes with the dataset)\n",
        "with open('wlasl_data/WLASL_v0.3.json', 'r') as f:\n",
        "    wlasl_data = json.load(f)\n",
        "\n",
        "# Create a mapping of video IDs to their gloss labels\n",
        "video_id_to_label = {}\n",
        "label_to_index = {}\n",
        "index = 0\n",
        "\n",
        "signs_count = 0\n",
        "for entry in wlasl_data:\n",
        "    if signs_count >= 2000:\n",
        "        break\n",
        "\n",
        "    gloss = entry['gloss']  # The sign word\n",
        "    if gloss not in label_to_index:\n",
        "        label_to_index[gloss] = index\n",
        "        index += 1\n",
        "        signs_count += 1\n",
        "\n",
        "    for instance in entry['instances']:\n",
        "        video_id = instance['video_id']\n",
        "        video_id_to_label[video_id] = gloss\n",
        "\n",
        "# Save the mappings for later use\n",
        "with open('video_id_to_label.json', 'w') as f:\n",
        "    json.dump(video_id_to_label, f)\n",
        "\n",
        "with open('label_to_index.json', 'w') as f:\n",
        "    json.dump(label_to_index, f)\n",
        "\n",
        "print(f\"Created mappings for {len(video_id_to_label)} videos across {len(label_to_index)} unique signs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKbOkYNm9gU7"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Uninstall any conflicting versions\n",
        "!pip uninstall -y mediapipe protobuf numpy\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel --quiet\n",
        "\n",
        "# STEP 2: Reinstall compatible versions\n",
        "!pip install mediapipe==0.10.9 protobuf==3.20.* numpy==1.23.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekgNv4qPYbHq"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip setuptools wheel --quiet\n",
        "# !pip install mediapipe==0.10.9 protobuf==3.20.3 numpy==1.23.5 --force-reinstall --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74_uz-8hW-rp"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "# Load mappings\n",
        "with open('video_id_to_label.json', 'r') as f:\n",
        "    video_id_to_label = json.load(f)\n",
        "\n",
        "with open('label_to_index.json', 'r') as f:\n",
        "    label_to_index = json.load(f)\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, lh, rh])\n",
        "\n",
        "def process_video(video_path, holistic, max_frames=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    sequence = []\n",
        "\n",
        "    while cap.isOpened() and len(sequence) < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        image.flags.writeable = False\n",
        "        results = holistic.process(image)\n",
        "        image.flags.writeable = True\n",
        "\n",
        "        keypoints = extract_keypoints(results)\n",
        "        sequence.append(keypoints)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    while len(sequence) < max_frames:\n",
        "        sequence.append(np.zeros(225))  # pad if needed\n",
        "\n",
        "    return np.array(sequence)\n",
        "\n",
        "# Process all videos\n",
        "input_dir = 'wlasl_data/videos'\n",
        "output_dir = 'keypoints_data'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    for video_id, label in tqdm(list(video_id_to_label.items())[:2000]):\n",
        "        video_path = os.path.join(input_dir, f\"{video_id}.mp4\")\n",
        "        if not os.path.exists(video_path):\n",
        "            continue\n",
        "\n",
        "        save_dir = os.path.join(output_dir, label)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        save_path = os.path.join(save_dir, f\"{video_id}.npy\")\n",
        "\n",
        "        if os.path.exists(save_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            sequence = process_video(video_path, holistic)\n",
        "            np.save(save_path, sequence)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process {video_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAeym8VX4RXr"
      },
      "outputs": [],
      "source": [
        "#do checking\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "keypoints_dir = 'keypoints_data'\n",
        "broken_files = []\n",
        "\n",
        "for label in os.listdir(keypoints_dir):\n",
        "    label_dir = os.path.join(keypoints_dir, label)\n",
        "    if not os.path.isdir(label_dir):\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(label_dir):\n",
        "        if file.endswith('.npy'):\n",
        "            file_path = os.path.join(label_dir, file)\n",
        "            try:\n",
        "                data = np.load(file_path)\n",
        "                if data.shape != (30, 225):\n",
        "                    print(f\"Unexpected shape in {file_path}: {data.shape}\")\n",
        "                    broken_files.append(file_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load {file_path}: {e}\")\n",
        "                broken_files.append(file_path)\n",
        "\n",
        "print(f\"\\nTotal broken or invalid files: {len(broken_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1nPC70C1a5R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "keypoints_dir = 'keypoints_data'  # where the npy files are saved\n",
        "\n",
        "labels = sorted(os.listdir(keypoints_dir))\n",
        "label_to_index = {label: i for i, label in enumerate(labels)}\n",
        "index_to_label = {str(i): label for label, i in label_to_index.items()}\n",
        "\n",
        "# Save label mappings\n",
        "with open('label_to_index.json', 'w') as f:\n",
        "    json.dump(label_to_index, f)\n",
        "\n",
        "with open('index_to_label.json', 'w') as f:\n",
        "    json.dump(index_to_label, f)\n",
        "\n",
        "# Create video_id to label mappings\n",
        "video_id_to_label = {}\n",
        "\n",
        "for label in labels:\n",
        "    label_path = os.path.join(keypoints_dir, label)\n",
        "    for file in os.listdir(label_path):\n",
        "        if file.endswith('.npy'):\n",
        "            video_id = file.replace('.npy', '')\n",
        "            video_id_to_label[video_id] = label\n",
        "\n",
        "with open('video_id_to_label.json', 'w') as f:\n",
        "    json.dump(video_id_to_label, f)\n",
        "\n",
        "print(f\"Total labels: {len(label_to_index)}\")\n",
        "print(f\"Total videos: {len(video_id_to_label)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_file = os.path.join(keypoints_dir, label, os.listdir(os.path.join(keypoints_dir, label))[0])\n",
        "keypoints = np.load(sample_file)\n",
        "\n",
        "# Plot hand X coordinates over time for first 30 frames\n",
        "plt.plot(keypoints[:, 99:120:3])  # right hand x-coordinates (e.g., landmarks 0–6)\n",
        "plt.title('Right hand keypoints (X) over 30 frames')\n",
        "plt.xlabel('Frame')\n",
        "plt.ylabel('X position')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "phM5i1jAUJTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for label in os.listdir(keypoints_dir):\n",
        "    label_dir = os.path.join(keypoints_dir, label)\n",
        "    if os.path.isdir(label_dir):\n",
        "        count = len([f for f in os.listdir(label_dir) if f.endswith('.npy')])\n",
        "        label_counts[label] = count\n",
        "\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"{label}: {count} samples\")\n"
      ],
      "metadata": {
        "id": "tYGF0500YtTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = np.load(os.path.join(keypoints_dir, label, os.listdir(os.path.join(keypoints_dir, label))[0]))\n",
        "print(sample.shape)      # Should be (30, 225)\n",
        "print(sample[0][:10])    # Print first frame, first 10 features\n"
      ],
      "metadata": {
        "id": "0Edr_wmdYzMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the x coordinates of the first 10 landmarks in the first frame\n",
        "plt.plot(sample[0][:30:3])  # x values only\n",
        "plt.title(\"X Coordinates of First 10 Keypoints in Frame 0\")\n",
        "plt.xlabel(\"Keypoint Index\")\n",
        "plt.ylabel(\"X value\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CfIVOHeEZ90C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "DATA_PATH = 'keypoints_data'  # change to your actual path\n",
        "SEQUENCE_LENGTH = 30         # should match what you extracted\n",
        "FEATURE_DIM = 225            # should match keypoint length\n",
        "\n",
        "labels = sorted(os.listdir(DATA_PATH))\n",
        "label_map = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "sequences, labels_list = [], []\n",
        "\n",
        "for label in labels:\n",
        "    label_dir = os.path.join(DATA_PATH, label)\n",
        "    for file in os.listdir(label_dir):\n",
        "        if file.endswith('.npy'):\n",
        "            path = os.path.join(label_dir, file)\n",
        "            sequence = np.load(path)\n",
        "            if sequence.shape == (SEQUENCE_LENGTH, FEATURE_DIM):\n",
        "                sequences.append(sequence)\n",
        "                labels_list.append(label_map[label])\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = to_categorical(labels_list)\n",
        "\n",
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n"
      ],
      "metadata": {
        "id": "r-QV3Is4eDcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def test_and_split(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and validation sets.\n",
        "\n",
        "    Parameters:\n",
        "    - X (np.array): Input features of shape (samples, sequence_length, feature_dim)\n",
        "    - y (np.array): One-hot encoded labels of shape (samples, num_classes)\n",
        "    - test_size (float): Proportion of the dataset to include in the validation split\n",
        "    - random_state (int): Seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_val, y_train, y_val: Split datasets\n",
        "    \"\"\"\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=y.argmax(axis=1) if y.shape[1] > 1 else None  # stratify by class index\n",
        "    )\n",
        "    return X_train, X_val, y_train, y_val\n"
      ],
      "metadata": {
        "id": "bHDZ7VUGefu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = test_and_split(X, y)\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation shape: {X_val.shape}, {y_val.shape}\")"
      ],
      "metadata": {
        "id": "XyICSLOnesO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = sorted(os.listdir('keypoints_data'))\n",
        "label_map = {label: idx for idx, label in enumerate(actions)}\n"
      ],
      "metadata": {
        "id": "00ELcCGWw7aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y if y.shape[1] > 1 else None\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "qJVxpkAKg4Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "metadata": {
        "id": "uIQTvZeVe6-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "8yfufJAde_Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM MODEL\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,225)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(98, activation='softmax'))"
      ],
      "metadata": {
        "id": "iEyDA-MRfCF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "k0SkCklofunG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "OI67NOWgfxEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=500, callbacks=[tb_callback])"
      ],
      "metadata": {
        "id": "EB46_ZfjkyL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN MODEL\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1D Convolution over time steps (frames)\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(30, 225)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(y.shape[1], activation='softmax'))  # Number of classes\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "a2ip6eWdo2YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Regularization factor\n",
        "l2_reg = 0.001\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First Conv1D block\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
        "                 kernel_regularizer=l2(l2_reg), input_shape=(30, 225)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))  # Add dropout after pooling\n",
        "\n",
        "# Second Conv1D block\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu',\n",
        "                 kernel_regularizer=l2(l2_reg)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))  # Dropout after second pooling\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "wFVWc-AQ3haQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200,\n",
        "          validation_data=(X_val, y_val),\n",
        "          callbacks=[tb_callback, early_stop])\n"
      ],
      "metadata": {
        "id": "GF15TlGfuEt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir Logs/fit"
      ],
      "metadata": {
        "id": "d-zNeJB45bJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 10   # 可以换成其他索引\n",
        "\n",
        "predicted_label = actions[np.argmax(res[i])]\n",
        "actual_label = actions[np.argmax(y_test[i])]\n",
        "\n",
        "print(f\"Sample {i} → Predicted: {predicted_label}, Actual: {actual_label}\")\n"
      ],
      "metadata": {
        "id": "Cte4cwKZhd4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "3Sjy7b_qweI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.predict(X_test)"
      ],
      "metadata": {
        "id": "cSyLgJ_NwgWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrue = np.argmax(y_test, axis=1).tolist()\n",
        "yhat = np.argmax(yhat, axis=1).tolist()"
      ],
      "metadata": {
        "id": "k7JbzYsFwgSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilabel_confusion_matrix(ytrue, yhat)"
      ],
      "metadata": {
        "id": "hAFDNPV-wkph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(ytrue, yhat)"
      ],
      "metadata": {
        "id": "p9MKyj-twpk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5n6F8ynW5Vc"
      },
      "source": [
        "IGNORE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX47eDv_W8WH"
      },
      "source": [
        "IGNORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvFiMKXBQuEe"
      },
      "outputs": [],
      "source": [
        "#2-------------------------------------\n",
        "def extract_frames(video_path, output_dir, video_id, sample_rate=10):\n",
        "    \"\"\"\n",
        "    Extract frames from a video file\n",
        "    sample_rate: extract 1 frame per this many frames\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % sample_rate == 0:\n",
        "            # Preprocess frame\n",
        "            frame = cv2.resize(frame, (224, 224))  # Resize for CNN\n",
        "            save_path = os.path.join(output_dir, f\"{video_id}_{saved_count:04d}.jpg\")\n",
        "            cv2.imwrite(save_path, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return saved_count\n",
        "\n",
        "# Process the first 2000 videos\n",
        "videos_dir = 'wlasl_data/videos'\n",
        "frames_output_dir = 'extracted_frames/'\n",
        "\n",
        "# Create a dataframe to keep track of processed data\n",
        "data_info = []\n",
        "\n",
        "for video_id, label in tqdm(list(video_id_to_label.items())[:500]):\n",
        "    video_path = os.path.join(videos_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "    if os.path.exists(video_path):\n",
        "        output_subdir = os.path.join(frames_output_dir, label)\n",
        "        frame_count = extract_frames(video_path, output_subdir, video_id)\n",
        "\n",
        "        if frame_count > 0:\n",
        "            data_info.append({\n",
        "                'video_id': video_id,\n",
        "                'label': label,\n",
        "                'frame_count': frame_count,\n",
        "                'label_index': label_to_index[label]\n",
        "            })\n",
        "\n",
        "# Save the dataset info\n",
        "pd.DataFrame(data_info).to_csv('dataset_info.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni1plBFNScKz"
      },
      "outputs": [],
      "source": [
        "#3-------------------------------------\n",
        "import os\n",
        "\n",
        "frames_dir = 'extracted_frames/'\n",
        "if os.path.exists(frames_dir):\n",
        "    # Count directories (labels)\n",
        "    labels = [d for d in os.listdir(frames_dir) if os.path.isdir(os.path.join(frames_dir, d))]\n",
        "    print(f\"Found {len(labels)} label directories\")\n",
        "\n",
        "    # Count total frames\n",
        "    total_frames = 0\n",
        "    for label in labels:\n",
        "        label_dir = os.path.join(frames_dir, label)\n",
        "        frames = [f for f in os.listdir(label_dir) if f.endswith(('.jpg', '.png'))]\n",
        "        total_frames += len(frames)\n",
        "        print(f\"  - {label}: {len(frames)} frames\")\n",
        "\n",
        "    print(f\"Total extracted frames: {total_frames}\")\n",
        "else:\n",
        "    print(\"Extracted frames directory not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRHdn7j4TmeT"
      },
      "outputs": [],
      "source": [
        "#4-------------------------------------\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load your dataset information\n",
        "df = pd.read_csv('dataset_info.csv')\n",
        "print(f\"Loaded dataset info with {len(df)} entries\")\n",
        "\n",
        "# Get unique labels from the dataset\n",
        "unique_labels = df['label'].unique()\n",
        "num_classes = len(unique_labels)\n",
        "print(f\"Found {num_classes} unique classes\")\n",
        "\n",
        "# Create label mappings from scratch\n",
        "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "index_to_label = {str(i): label for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Create a direct mapping from video_id to label\n",
        "video_id_to_label = dict(zip(df['video_id'], df['label']))\n",
        "video_id_to_label_index = {video_id: label_to_index[label] for video_id, label in video_id_to_label.items()}\n",
        "\n",
        "# 2. Load the label mapping\n",
        "with open('label_to_index.json', 'w') as f:\n",
        "    json.dump(label_to_index, f)\n",
        "\n",
        "with open('index_to_label.json', 'w') as f:\n",
        "    json.dump(index_to_label, f)\n",
        "\n",
        "print(\"\\nVerifying mappings:\")\n",
        "for video_id in list(df['video_id'])[:10]:\n",
        "    label = video_id_to_label[video_id]\n",
        "    label_index = label_to_index[label]\n",
        "    recovered_label = index_to_label[str(label_index)]\n",
        "    print(f\"Video ID: {video_id}, Label: {label}, Index: {label_index}, Recovered Label: {recovered_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YQ7pqmLuaLu"
      },
      "outputs": [],
      "source": [
        "# Split into train and validation\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'] if len(df['label'].unique()) > 1 else None)\n",
        "train_videos = train_df['video_id'].tolist()\n",
        "val_videos = val_df['video_id'].tolist()\n",
        "\n",
        "print(f\"\\nTraining set: {len(train_videos)} videos\")\n",
        "print(f\"Validation set: {len(val_videos)} videos\")\n",
        "\n",
        "# Check if frames exist for a few random videos\n",
        "print(\"\\nChecking for frames:\")\n",
        "frames_dir = 'extracted_frames'\n",
        "for video_id in train_videos[:5]:\n",
        "    # Get label for this video\n",
        "    label = video_id_to_label[video_id]\n",
        "    frame_dir = os.path.join(frames_dir, label)\n",
        "\n",
        "    if os.path.exists(frame_dir):\n",
        "        frames = [f for f in os.listdir(frame_dir) if f.startswith(str(video_id))]\n",
        "        print(f\"Video {video_id} (label: {label}): {len(frames)} frames\")\n",
        "    else:\n",
        "        print(f\"ERROR: Directory for label {label} not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd5IqOctUDiJ"
      },
      "outputs": [],
      "source": [
        "# 3. Set up data generators\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Create a dictionary mapping video IDs to the label index\n",
        "video_id_to_label_index = dict(zip(df['video_id'], df['label_index']))\n",
        "\n",
        "# Define a function to generate data from our frames\n",
        "def frame_generator(video_ids, batch_size):\n",
        "    while True:\n",
        "        # Shuffle the video IDs for each epoch\n",
        "        np.random.shuffle(video_ids)\n",
        "\n",
        "        for i in range(0, len(video_ids), batch_size):\n",
        "            batch_ids = video_ids[i:i+batch_size]\n",
        "            batch_x = []\n",
        "            batch_y = []\n",
        "\n",
        "            for video_id in batch_ids:\n",
        "                # Get label for this video\n",
        "                label_index = video_id_to_label_index[video_id]\n",
        "\n",
        "                # Get all frames for this video\n",
        "                label = index_to_label[str(label_index)]\n",
        "                frame_dir = os.path.join('extracted_frames', label)\n",
        "                frames = [f for f in os.listdir(frame_dir) if f.startswith(str(video_id))]\n",
        "\n",
        "                if frames:\n",
        "                    # Select a random frame from this video\n",
        "                    frame_path = os.path.join(frame_dir, np.random.choice(frames))\n",
        "                    img = tf.keras.preprocessing.image.load_img(\n",
        "                        frame_path, target_size=IMAGE_SIZE)\n",
        "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "                    img_array = img_array / 255.0  # Normalize\n",
        "\n",
        "                    batch_x.append(img_array)\n",
        "                    batch_y.append(label_index)\n",
        "\n",
        "            if batch_x:\n",
        "                yield np.array(batch_x), tf.keras.utils.to_categorical(batch_y, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBUrM7HjUH8G"
      },
      "outputs": [],
      "source": [
        "# Split data into training and validation sets\n",
        "train_videos, val_videos = train_test_split(df['video_id'].tolist(), test_size=0.2, random_state=42)\n",
        "\n",
        "# Create generators\n",
        "train_gen = frame_generator(train_videos, BATCH_SIZE)\n",
        "val_gen = frame_generator(val_videos, BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['video_id'].tolist())"
      ],
      "metadata": {
        "id": "nVEWSM-BdJMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kJNbKhjY1mn"
      },
      "outputs": [],
      "source": [
        "# First, check if we have any data to work with\n",
        "print(f\"Total video IDs available: {len(df)}\")\n",
        "print(f\"Training videos: {len(train_videos)}\")\n",
        "print(f\"Validation videos: {len(val_videos)}\")\n",
        "\n",
        "# Check if frames directory exists\n",
        "frames_dir = 'extracted_frames'\n",
        "if not os.path.exists(frames_dir):\n",
        "    print(f\"ERROR: Frames directory '{frames_dir}' doesn't exist!\")\n",
        "else:\n",
        "    # Check contents of the frames directory\n",
        "    labels = [d for d in os.listdir(frames_dir) if os.path.isdir(os.path.join(frames_dir, d))]\n",
        "    print(f\"Found {len(labels)} label directories in {frames_dir}\")\n",
        "\n",
        "    # Check if frames exist for a few random videos\n",
        "    for video_id in train_videos[:5]:\n",
        "        # Get label for this video\n",
        "        label_index = video_id_to_label_index[video_id]\n",
        "        label = index_to_label[str(label_index)]\n",
        "        frame_dir = os.path.join(frames_dir, label)\n",
        "\n",
        "        if os.path.exists(frame_dir):\n",
        "            frames = [f for f in os.listdir(frame_dir) if f.startswith(str(video_id))]\n",
        "            print(f\"Video {video_id} (label: {label}): {len(frames)} frames\")\n",
        "        else:\n",
        "            print(f\"ERROR: Directory for label {label} not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PQuQoASjUKWe"
      },
      "outputs": [],
      "source": [
        "# 4. Building the CNN model\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=0.001,\n",
        "        weight_decay=0.01,\n",
        "    ),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Train the model\n",
        "steps_per_epoch = len(train_videos) // BATCH_SIZE\n",
        "validation_steps = max(1, len(val_videos) // BATCH_SIZE)\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=30,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beNQRe_VeYaY"
      },
      "outputs": [],
      "source": [
        "# 6. Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYlfIyHz__fm"
      },
      "outputs": [],
      "source": [
        "#New CNN Model after batch normalization and decrease the dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Using transfer learning with MobileNetV2 for better performance\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adamW',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Train the model\n",
        "steps_per_epoch = len(train_videos) // BATCH_SIZE\n",
        "validation_steps = max(1, len(val_videos) // BATCH_SIZE)\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=30,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEOLZWihKNOl"
      },
      "outputs": [],
      "source": [
        "print(\"Number of classes:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T3kQlh8r4S7Z"
      },
      "outputs": [],
      "source": [
        "# 6. Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "10TbEr8BUQPC"
      },
      "outputs": [],
      "source": [
        "# 7. Fine-tune the model by unfreezing some layers\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=30,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=validation_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5E62pEjUTkE"
      },
      "outputs": [],
      "source": [
        "# 8. Save the model\n",
        "import time\n",
        "model_name = f'/content/drive/MyDrive/cnn_model_{int(time.time())}.keras'\n",
        "model.save(model_name)\n",
        "\n",
        "# Save the class labels mapping\n",
        "with open('index_to_label.json', 'w') as f:\n",
        "    json.dump(index_to_label, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_v6ZPv126iW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWGeyl-tUW3v"
      },
      "outputs": [],
      "source": [
        "# 9. Convert to TensorFlow Lite for mobile or edge deployment\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('sign_language_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model training complete and saved!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "10qxBYXZm7ns9ZiYzr5Yq2eORCkuOXIfT",
      "authorship_tag": "ABX9TyM3sFH40Ho/nAPPmBcIdiNd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}